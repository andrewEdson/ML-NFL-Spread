# ğŸˆ NFL Spread Prediction ML Project

A comprehensive machine learning project to predict NFL point spreads using team performance statistics, injury data, and betting information. The ensemble of **4 different models** achieves consistent performance with proper chronological validation to prevent data leakage.

## ğŸ¯ Project Overview

This project predicts whether the home team will cover the betting spread for NFL games using:

- **Team offensive/defensive statistics** (passing, rushing, receiving)
- **Historical performance metrics** with rolling averages
- **Injury data integration** (2009+) with position-specific severity mapping
- **Betting market data** (spreads, moneylines, totals)
- **Game context** (weather, rest days, coaches, referees)
- **Advanced analytics** with SHAP explainability and ensemble methods

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone <repository-url>
cd ML-NFL-Spread

# Create virtual environment
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Train Multiple Models

```bash
# Train all models with injury data integration
python scripts/model_scripts/xgb_train.py      # XGBoost
python scripts/model_scripts/rf_train.py       # Random Forest
python scripts/model_scripts/lgbm_train.py     # LightGBM
python scripts/model_scripts/cat_boost_train.py # CatBoost
```

### 3. Analyze Model Performance

```bash
# Compare all 4 models with feature importance
python evaluation/eval.py

# Deep SHAP analysis for explainability
python evaluation/shap_eval.py

# Run ensemble predictions with all models
python evaluation/ensemble_test.py
```

## ğŸ“Š Model Performance

| Model             | Accuracy   | Training Period | Test Period | Specialized Strengths         |
| ----------------- | ---------- | --------------- | ----------- | ----------------------------- |
| **Random Forest** | **53.96%** | 1999-2021       | 2022-2023   | Personnel & coaching factors  |
| **LightGBM**      | 52.12%     | 1999-2021       | 2022-2023   | Gradient boosting efficiency  |
| **CatBoost**      | 52.12%     | 1999-2021       | 2022-2023   | Native categorical handling   |
| **XGBoost**       | 51.93%     | 1999-2021       | 2022-2023   | Feature interaction discovery |
| **Ensemble**      | 51.38%     | All 4 Models    | 2022-2023   | Averaged predictions          |

### ğŸ”¬ Advanced Analytics

- **SHAP Analysis**: Model explainability with feature importance visualization
- **Ensemble Methods**: 4-model averaging for robust predictions
- **Injury Integration**: Position-specific injury severity mapping (2009+)
- **Feature Comparison**: Zero overlap in top features between models

_Note: Uses chronological validation (no data leakage) - predicting actual future games_

## âš ï¸ Data Leakage Discovery & Fix

### The Problem

Initially, the models achieved **~78% accuracy** using random train/test splits, which seemed too good to be true. Upon investigation, I discovered **data leakage**:

- Using team stats from the **same week** as the game being predicted
- Random splits allowed the model to "peek" at future information
- This created unrealistic performance that wouldn't work in practice

### The Solution

1. **Time-shifted features**: Shifted all team statistics by 1 week (only use past performance)
2. **Chronological validation**: Train on 1999-2021, test on 2022-2023 (no future data)
3. **Rolling averages**: Use 3-week rolling means instead of single-game stats

**Result**: More realistic 53% accuracy that represents genuine predictive power for future games.

## ğŸ—ï¸ Architecture

### Data Pipeline

1. **Raw Data**: NFL game schedules + player statistics via `nfl-data-py`
2. **Injury Integration**: Position-specific injury mapping with severity weights
3. **Team Aggregation**: Player stats â†’ team-level weekly performance
4. **Time-shift**: Prevent data leakage by using only past performance
5. **Rolling Features**: 3-week rolling averages for trend analysis
6. **Feature Engineering**: 78+ features including injury metrics and betting data

### Multi-Model Approach

**4 Specialized Models:**

1. **Random Forest**: Personnel-focused (QB names, coaches, referees)
2. **LightGBM**: Efficient gradient boosting with feature interaction
3. **CatBoost**: Native categorical feature handling (no encoding needed)
4. **XGBoost**: Advanced boosting with hyperparameter optimization

**Ensemble Strategy:**

- Simple averaging of all 4 model predictions
- Categorical feature decoding for CatBoost compatibility
- Real test data validation (543 games from 2022-2023)

### Key Features (Built-in Model Importance)

**Random Forest prioritizes:**

1. **Personnel factors** - QB names, coaches, referees (encoded)
2. **Betting market signals** - Total line, spread line
3. **Advanced metrics** - EPA (Expected Points Added)
4. **Rushing efficiency** - Ground game performance

**XGBoost prioritizes:**

1. **Receiving performance** - Passing game metrics, TDs away
2. **Turnover statistics** - Interceptions and fumbles
3. **Injury factors** - Position-specific injury metrics
4. **Coaching impact** - Home coach encoded

**LightGBM prioritizes:**

1. **Personnel factors** - QB and coach encoded features
2. **Team efficiency** - Passing EPA, rushing yards
3. **Market context** - Total line betting information
4. **Game flow** - Fantasy points, EPA metrics

**CatBoost prioritizes:**

1. **Categorical variables** - Raw QB names, coaches, referees
2. **Team identity** - Home/away team categories
3. **Efficiency metrics** - EPA and rushing performance
4. **Betting context** - Spread and total lines

### Model Architecture

- **Multi-Model Ensemble** with 4 specialized algorithms
- **78 engineered features** including injury metrics and betting data
- **Chronological train/test split** (1999-2021 train, 2022-2023 test)
- **Data leak prevention** via time-shifted features
- **SHAP explainability** for model transparency
- **Categorical handling** with both encoding and native approaches

## ğŸ“ Project Structure

```
ML-NFL-Spread/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ model_scripts/
â”‚   â”‚   â”œâ”€â”€ xgb_train.py      # XGBoost training script
â”‚   â”‚   â”œâ”€â”€ rf_train.py       # Random Forest training script
â”‚   â”‚   â”œâ”€â”€ lgbm_train.py     # LightGBM training script
â”‚   â”‚   â””â”€â”€ cat_boost_train.py # CatBoost training script
â”‚   â”œâ”€â”€ data_scripts/
â”‚   â”‚   â””â”€â”€ download_data.py  # Data preprocessing pipeline
â”‚   â””â”€â”€ helpful/
â”‚       â”œâ”€â”€ train.py          # Legacy training script
â”‚       â”œâ”€â”€ train_from_processed.py  # Fast training from saved data
â”‚       â””â”€â”€ columns.py        # Data exploration utilities
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval.py              # Multi-model comparison
â”‚   â”œâ”€â”€ shap_eval.py         # SHAP explainability analysis
â”‚   â””â”€â”€ ensemble_test.py     # 4-model ensemble evaluation
â”œâ”€â”€ models/                  # Trained model files (.pkl, .cbm)
â”œâ”€â”€ data/                   # Processed datasets (gitignored)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Key Scripts

### Training Scripts

- **`xgb_train.py`**: XGBoost with hyperparameter optimization
- **`rf_train.py`**: Random Forest with personnel focus
- **`lgbm_train.py`**: LightGBM for efficient gradient boosting
- **`cat_boost_train.py`**: CatBoost with native categorical handling
- **`download_data.py`**: Complete data preprocessing pipeline

### Analysis Scripts

- **`eval.py`**: Multi-model feature importance comparison
- **`shap_eval.py`**: SHAP explainability analysis with visualizations
- **`ensemble_test.py`**: 4-model ensemble evaluation on real test data
- **`columns.py`**: Data exploration and column analysis

## ğŸ“ˆ Feature Engineering

### Data Leak Prevention

- **Time-shifted features**: Use only past performance (shift by 1 week)
- **Rolling averages**: 3-week rolling means for trend analysis
- **Chronological splits**: Train on past, test on future

### Advanced Features

- **Injury Analytics**: Position-specific injury severity mapping (QB=5, RB=3, etc.)
- **Team Statistics**: Aggregated from player-level data with EPA metrics
- **Betting Context**: Spreads, moneylines, totals with market efficiency
- **Environmental**: Weather, temperature, wind conditions
- **Personnel**: Coaches, referees, quarterbacks (both encoded and categorical)
- **Rest/Travel**: Days between games, travel patterns

### Categorical Feature Handling

- **Traditional Models**: LabelEncoder for personnel data (XGBoost, Random Forest, LightGBM)
- **CatBoost**: Native categorical features without encoding
- **Ensemble Compatibility**: Automatic categorical decoding for CatBoost integration

## ğŸ¯ Model Insights & Feature Analysis

### Multi-Model Strategy Discovery

Each model learned **completely different patterns** with zero overlap in top 10 features, suggesting strong **ensemble potential**:

**Random Forest Strategy:**

- **Personnel-driven**: QB performance, coaching impact, referee influence
- **Market context**: Betting lines and spreads
- **Advanced efficiency**: EPA metrics for all phases

**XGBoost Strategy:**

- **Game flow focus**: Receiving performance, turnover statistics
- **Injury impact**: Position-specific injury metrics
- **Situational factors**: Coaching and rest advantages

**LightGBM Strategy:**

- **Efficiency metrics**: EPA-based features across all game phases
- **Personnel factors**: Encoded QB and coaching features
- **Market signals**: Total line betting information

**CatBoost Strategy:**

- **Raw categorical power**: Unencoded QB names, coaches, referees
- **Team identity**: Direct team name importance
- **Efficiency focus**: EPA and rushing performance metrics

### SHAP Explainability

- **Model transparency**: SHAP values for every prediction
- **Feature contribution**: Understand why models make specific predictions
- **Comparison analysis**: Visualize feature importance differences across models
- **Permutation importance**: Fallback analysis for unsupported models

### Key Discovery

**Zero feature overlap** in top 10 across all 4 models demonstrates **complementary learning** - each algorithm finds value in different aspects of NFL games, making them ideal for ensemble approaches.

## ğŸ”® Future Improvements

- [ ] **Weighted Ensemble**: Performance-based model weighting instead of simple averaging
- [ ] **Advanced Injury Models**: Predict injury impact on team performance
- [ ] **Real-time Pipeline**: Live predictions for current NFL season
- [ ] **Deep Learning**: Neural networks with embedding layers for categorical data
- [ ] **Bayesian Optimization**: Automated hyperparameter tuning across all models
- [ ] **Stacking Ensemble**: Meta-learner to combine model predictions optimally

## ğŸ† Results Summary

This project demonstrates **advanced NFL spread prediction** with:

- âœ… **53.96% accuracy** with Random Forest (best individual model)
- âœ… **4-model ensemble** with complementary learning strategies
- âœ… **SHAP explainability** for transparent model decisions
- âœ… **Injury integration** with position-specific severity mapping
- âœ… **Proper validation** methodology preventing overfitting
- âœ… **Production-ready** pipeline with comprehensive model comparison
- âœ… **78+ engineered features** from 25+ years of NFL data

**Note**: 53.96% accuracy represents significant edge in sports betting (>1.5% above break-even rate of ~52.4%)

## ğŸ“‹ Dependencies

See `requirements.txt` for full list. Key packages:

- **Data**: `nfl-data-py`, `pandas`, `numpy`
- **ML Models**: `scikit-learn`, `xgboost`, `lightgbm`, `catboost`
- **Analysis**: `matplotlib`, `seaborn`, `shap`
- **Utils**: `joblib`, `fastparquet`, `pickle`

## ğŸ“§ Contact

Created for NFL spread prediction research and education.

---

_Disclaimer: This project is for educational purposes. Sports betting involves risk._
